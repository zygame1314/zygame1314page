<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
    <title>解密必应“无法索引”之谜：Cloudflare Pages + CDN + Functions 预渲染 SEO 终极指南 - zygame1314</title>
</head>

<body>
    <main class="article-content">
        <div class="article-body">
            <h1>解密必应“无法索引”之谜：Cloudflare Pages + CDN + Functions 预渲染 SEO 终极指南</h1>
            <p class="article-intro">
                各位访客们，有没有遇到过这样的情况：你的博客在 Google 那里如鱼得水，收录秒速，排名稳健；可一到必应（Bing）这边，就给你来个“该 URL
                无法由必应进行索引编制，因为它是重定向链接”的闭门羹？我的博客 <a href="https://blog.zygame1314.site"
                    target="_blank">blog.zygame1314.site</a> 就遭遇了这般“特殊待遇”。明明只是加了个
                CDN，怎么到了必应老哥这里就这么“较真”呢？别急，且听我慢慢道来。
            </p>

            <h2>引子：神秘的必应“无法索引”警告</h2>
            <p>
                故事的开端，源于我在必应网站管理员工具中看到的一条刺眼提示。我的博客，<a href="https://blog.zygame1314.site"
                    target="_blank">blog.zygame1314.site</a>，部署在 Cloudflare Pages 上，为了国内访问速度，套了一层又拍云 CDN。Google Search
                Console 里一切安好，文章正常收录。然而，必应却无情地指出：“该 URL 无法由必应进行索引编制，因为它是重定向链接”，并且目标直指
                <code>https://zygame1314page.pages.dev/</code>。
            </p>
            <p>
                我当时就纳闷了，这 <code>.pages.dev</code> 是 Cloudflare Pages 的默认域名，我寻思着 CDN 应该是透明转发的呀，怎么就成了重定向了呢？必应老哥，你是不是有什么误会？
            </p>

            <h2>初步分析：理解 CDN 与重定向</h2>
            <p>
                在深入排查之前，我们得先搞清楚几个概念：
            </p>
            <ul>
                <li><strong>CDN 的理想工作方式：</strong>用户访问 CDN
                    边缘节点，边缘节点若有缓存则直接返回，若无则回源（到我们的源站）获取内容，缓存后再返回给用户。整个过程对用户和搜索引擎应该是透明的，URL 保持不变。</li>
                <li><strong>可能的重定向问题：</strong>如果 CDN 配置不当，或者源站本身存在某些重定向逻辑，就可能导致搜索引擎爬虫在访问你的域名时，被重定向到另一个 URL。</li>
                <li><strong>搜索引擎为何“嫌弃”重定向链接：</strong>
                    <ul>
                        <li><strong>规范化 (Canonicalization):</strong> 搜索引擎希望为每个独立内容找到一个“权威”的 URL。如果多个 URL
                            指向同一内容（例如通过重定向），搜索引擎需要选择一个进行索引。直接索引重定向链接本身意义不大。</li>
                        <li><strong>爬虫效率:</strong> 频繁的重定向会消耗爬虫的预算和时间。搜索引擎更喜欢直接抓取最终内容。</li>
                    </ul>
                </li>
                <li><strong><code>.pages.dev</code> 域名的性质：</strong>这是 Cloudflare Pages
                    为每个项目提供的默认子域名。虽然方便测试，但对于正式站点，我们通常会绑定自定义域名。必应直接看到了这个域名，说明在它看来，我的自定义域名最终“指向”了这个默认域名。</li>
            </ul>

            <h2>第一轮排查与调整：回源地址的优化</h2>
            <p>
                我的架构是这样的，得先捋一捋：
            </p>
            <div class="config-explanation">
                <h4><i class="fas fa-sitemap"></i> 我的博客架构</h4>
                <ul>
                    <li><strong>应用部署：</strong>Cloudflare Pages 负责托管我的博客应用（一个半静态博客，部分内容由 Cloudflare Functions 动态生成）。我为这个
                        Pages 项目绑定了一个自定义域名：<code>api.zygame1314.site</code>，这个域名主要作为后端 API 和 CDN 的源站。</li>
                    <li><strong>用户访问域名：</strong>访客通过 <code>blog.zygame1314.site</code> 访问博客。</li>
                    <li><strong>CDN 加速：</strong><code>blog.zygame1314.site</code> 使用又拍云 CDN 进行加速，特别是优化国内访问体验。</li>
                    <li><strong>回源逻辑（最初的痛点）：</strong>起初，在又拍云 CDN 的配置里，我将回源地址设置为了 Cloudflare Pages 为项目提供的默认域名，即 <span
                            class="secure-code">https://zygame1314page.pages.dev/</span>。现在回想起来，这很可能就是必应“不高兴”的直接原因——它认为我的主域名
                        <code>blog.zygame1314.site</code> 最终指向了一个看似“不相关”或“非权威”的 <code>.pages.dev</code> 链接。
                    </li>
                </ul>
            </div>
            <p>
                基于上面的分析，我意识到问题症结就在于又拍云 CDN 的回源配置。如果 CDN 回源到一个被搜索引擎认为是“跳板”或者“非最终目的地”的 URL（比如那个 <code>.pages.dev</code>
                域名），即便内容正确返回，也可能被标记为重定向。
            </p>
            <p>
                <strong>调整措施与深层解读：</strong> 我的调整是将又拍云 CDN 的回源地址从之前的 <span
                    class="secure-code">https://zygame1314page.pages.dev/</span> <strong>明确更改为</strong>我在 Cloudflare
                Pages 上为该项目设置的自定义域名 <code>api.zygame1314.site</code>。这一招果然奏效！
            </p>
            <p>
                为什么这样调整就有效了呢？我分析有两层原因：
            </p>
            <ul>
                <li><strong>自定义域名的“正统性”：</strong> <code>api.zygame1314.site</code> 作为 Cloudflare Pages
                    项目的自定义域名，它本身就是该项目的“官方”访问地址之一，Cloudflare 不会再将其重定向到 <code>.pages.dev</code>。这是技术层面的直接原因。</li>
                <li><strong>搜索引擎的“亲缘关系”判断（我的猜测）：</strong> 更为微妙的一点可能是，当必应爬虫通过 <code>blog.zygame1314.site</code>
                    访问，感知到内容实际来源于 <code>api.zygame1314.site</code>
                    时，由于这两个域名（<code>blog.<strong>zygame1314.site</strong></code> 和
                    <code>api.<strong>zygame1314.site</strong></code>）都属于同一个主域名
                    <code>zygame1314.site</code>，必应可能认为这是一种“内部的”、“合理的”资源指向，而不是一个跳往通用第三方平台域名（如
                    <code>.pages.dev</code>）的“外部重定向”。这种“沾亲带故”的关系，可能让必应更容易接受这个配置，不再将其视为需要警告的重定向。
                </li>
            </ul>
            <p>
                简而言之，让 CDN 回源到一个与主访问域名在逻辑上或域名结构上更“亲近”的自定义源站域名，似乎比回源到一个平台提供的通用域名更能获得搜索引擎的“信任票”。
            </p>
            <p>
                <strong>初步效果：</strong> 调整后，必应网站管理员工具里关于“重定向链接”的警告果然消失了！太棒了！我长舒一口气，但转念一想，故事可能还没结束... SEO 的水，深着呢！
            </p>

            <h2>深入挖掘：动态内容的 SEO 挑战 —— 预渲染的引入</h2>
            <p>
                解决了重定向的“误会”后，我以为可以高枕无忧了。然而，没过多久，我发现虽然必应不再报重定向错误，但对于由 JS 动态生成的文章页面，收录情况依然不理想。这让我意识到，问题可能更深一层。
            </p>
            <p>
                我的博客部分内容（比如文章详情页）是类似 SPA (单页应用) 的模式，或者是通过客户端 JavaScript 动态渲染的。对于这类页面，搜索引擎爬虫（尤其是那些不太擅长执行复杂 JS
                的爬虫）可能只能看到一个空的入口点 HTML 壳子，或者需要执行 JS 才能加载出完整内容。这对于 SEO 来说是大忌！
            </p>
            <p>
                <strong>解决方案：预渲染 (Prerendering)！</strong>
                幸运的是，Cloudflare Pages Functions 提供了一个强大的武器：中间件 (<code>_middleware.js</code>)。我可以在中间件里判断请求的 User-Agent：
            </p>
            <ul>
                <li>如果是普通用户浏览器，正常走 SPA 或客户端渲染逻辑。</li>
                <li>如果是搜索引擎爬虫，并且访问的是文章路径 (例如 <code>/article/...</code>)，则从
                    <code>env.ASSETS</code>（代表项目中静态部署的文件）中获取该文章的纯静态 HTML 文件 (例如
                    <code>/articles/content/${articleId}.html</code>) 并直接返回。
                </li>
            </ul>
            <p>
                这样，爬虫就能拿到完整的、渲染好的 HTML 内容，索引起来自然就顺畅多了。
            </p>

            <h2>调试：让预渲染中间件正确工作</h2>
            <p>
                理论很美好，现实很骨感。配好中间件后，我兴冲冲地用 <code>curl</code> 模拟必应爬虫访问文章页：
            </p>
            <pre><code class="language-bash">curl -A "Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)" https://blog.zygame1314.site/article/my-awesome-post</code></pre>
            <p>
                结果呢？返回的居然还是我的 SPA 入口文件 <code>index.html</code>！这不科学！
            </p>
            <p>
                <strong>日志驱动的调试过程：</strong>
                遇到这种“灵异事件”，唯一的办法就是——加日志！疯狂加日志！我在 <code>_middleware.js</code> 的每一步都加上了详细的 <code>console.log</code>，然后通过
                Cloudflare Dashboard 查看 Pages Function 的实时日志。
            </p>
            <div class="article-tips">
                <h3><i class="fas fa-lightbulb"></i> 调试小技巧</h3>
                <ul>
                    <li><strong>充分利用日志:</strong> 在 Cloudflare Pages Functions 中，<code>console.log()</code>
                        是你最好的朋友。它可以帮你追踪代码执行路径、变量值，快速定位问题。</li>
                    <li><strong>模拟爬虫:</strong> 使用 <code>curl -A "爬虫UserAgent" URL</code> 命令可以方便地模拟搜索引擎爬虫的访问，验证预渲染逻辑。
                    </li>
                </ul>
            </div>
            <p>
                一步步追踪日志，我终于找到了“元凶”。在尝试通过 <code>env.ASSETS.fetch()</code> 获取预渲染的静态文章 HTML 时，日志里赫然出现了一条错误：
            </p>
            <strong>TypeError: Fetch API cannot load: /articles/content/xxxxxxxx.html</strong>
            <p>
                <strong>关键突破点与解决方案：</strong>
                这个 <code>TypeError</code> 提示 <code>Fetch API</code> 无法加载指定的路径。经过一番研究和查阅 Cloudflare 文档（根本没查），我发现
                <code>env.ASSETS.fetch()</code> 期望的参数是一个<strong>包含完整 URL 的 <code>Request</code>
                    对象</strong>，或者一个<strong>完整的 URL 字符串</strong>（例如
                <code>https://your-site.com/path/to/asset.html</code>）。而我之前可能错误地只传递了一个相对路径字符串 (如
                <code>/articles/content/...html</code>)，或者构造的 <code>Request</code> 对象不规范。
            </p>
            <p>
                正确的做法是，基于当前请求的 <code>origin</code> (例如 <code>https://blog.zygame1314.site</code>) 和预渲染文件的相对路径，构造一个完整的
                URL。这可以通过 <code>new URL(relativePath, request.url.origin)</code> 来实现。
            </p>
            <p>
                修改后的核心逻辑（已包含在下面的完整中间件代码中）：
            </p>
            <pre><code class="language-javascript">// 错误示例，可能导致 TypeError
// const articleResponse = await env.ASSETS.fetch(articleAssetRelativePath);

// 正确做法
const assetFullUrl = new URL(articleAssetRelativePath, request.url.origin).toString();
const articleResponse = await env.ASSETS.fetch(new Request(assetFullUrl));</code></pre>
            <p>
                <strong>还有个重点：</strong> 在成功获取并返回预渲染内容后，必须使用 <code>return new Response(...)</code>
                来<strong>终止中间件的执行</strong>。否则，代码会继续执行到最后的 <code>return next()</code>，导致又回到了 SPA 的怀抱，前功尽弃！
            </p>
            <p>以下是我最终的 <code>_middleware.js</code> (位于项目根目录或 <code>functions</code> 目录下，取决于你的项目结构)：</p>
            <pre><code class="language-javascript">
export async function onRequest(context) {
  const { request, next, env } = context;
  const rawUserAgent = request.headers.get('User-Agent') || '无User-Agent';
  const url = new URL(request.url);
  const pathname = url.pathname;

  console.log(`[中间件开始] 请求路径: ${pathname}, User-Agent: ${rawUserAgent}, 完整URL: ${request.url}`);

  const crawlerUserAgents = [
    'Googlebot', 'Bingbot', 'Slurp', 'DuckDuckBot', 'Baiduspider',
    'YandexBot', 'Sogou', 'Exabot', 'facebot', 'facebookexternalhit',
    'ia_archiver', 'LinkedInBot', 'Twitterbot', 'Pinterestbot'
  ];

  const isCrawler = crawlerUserAgents.some(crawler => rawUserAgent.toLowerCase().includes(crawler.toLowerCase()));
  console.log(`[中间件信息] 是否为爬虫: ${isCrawler}`);

  if (isCrawler && pathname.startsWith('/article/')) {
    console.log(`[中间件信息] 检测到爬虫访问文章路径: ${pathname}`);
    const articleId = pathname.substring('/article/'.length).split('?')[0].split('#')[0];
    console.log(`[中间件信息] 提取的文章ID: ${articleId}`);

    // 基本的路径安全检查，防止路径遍历等
    if (articleId && !articleId.includes('..') && !articleId.includes('/')) {
      let articleAssetRelativePath = ''; // 在 try 外部声明，以便 catch 中也能访问
      try {
        articleAssetRelativePath = `/articles/content/${articleId}.html`;
        console.log(`[中间件信息] 尝试从相对路径获取静态资源: ${articleAssetRelativePath}`);
        
        // 使用 request.url.origin 来确保我们总是基于当前请求的域名构造 URL
        // 例如，如果通过 blog.zygame1314.site 访问，origin 就是 https://blog.zygame1314.site
        // 如果是通过 api.zygame1314.site 访问（例如CDN回源），origin 就是 https://api.zygame1314.site
        // env.ASSETS.fetch 需要一个完整的 URL 或一个 Request 对象。
        const assetFullUrl = new URL(articleAssetRelativePath, url.origin).toString();
        console.log(`[中间件信息] 构建的静态资源完整URL: ${assetFullUrl}`);

        // 必须传递一个 Request 对象或完整的 URL 字符串给 env.ASSETS.fetch
        const articleResponse = await env.ASSETS.fetch(new Request(assetFullUrl));
        console.log(`[中间件信息] 文章静态资源获取状态 (${assetFullUrl}): ${articleResponse.status}`);

        if (articleResponse.ok) {
          console.log(`[中间件成功] 成功获取文章静态资源 ${assetFullUrl}。正在返回预渲染内容。`);
          const responseHeaders = new Headers(articleResponse.headers);
          responseHeaders.set('Content-Type', 'text/html; charset=utf-8');
          // 成功找到预渲染内容，直接返回，终止中间件
          return new Response(articleResponse.body, {
            headers: responseHeaders,
            status: 200
          });
        } else if (articleResponse.status === 404) {
          console.log(`[中间件警告] 未找到文章静态资源 ${assetFullUrl}。尝试提供自定义404页面。`);
          try {
            const custom404PagePath = "/custom/404.html"; // 确保这个文件实际存在于你的项目中
            const custom404FullUrl = new URL(custom404PagePath, url.origin).toString();
            console.log(`[中间件信息] 尝试获取自定义404页面: ${custom404FullUrl}`);
            const custom404Response = await env.ASSETS.fetch(new Request(custom404FullUrl));

            if (custom404Response.ok) {
              console.log(`[中间件成功] 找到自定义404页面。以404状态码提供其内容。`);
              const responseHeaders = new Headers(custom404Response.headers);
              responseHeaders.set('Content-Type', 'text/html; charset=utf-8');
              // 返回自定义404页面，并设置正确的404状态码
              return new Response(custom404Response.body, {
                headers: responseHeaders,
                status: 404
              });
            } else {
              console.warn(`[中间件警告] 自定义404页面 (${custom404FullUrl}) 获取失败，状态码: ${custom404Response.status}。将返回通用404文本。`);
              return new Response('页面未找到 (自定义404页面亦不可用)', {
                status: 404,
                headers: { 'Content-Type': 'text/plain; charset=utf-8' }
              });
            }
          } catch (custom404Error) {
            console.error(`[中间件错误] 获取自定义404页面时发生异常:`, custom404Error.message, custom404Error.stack);
            // 即使处理404页面也出错了，也要返回一个404响应
            return new Response('页面未找到 (处理自定义404页面时发生错误)', {
              status: 404,
              headers: { 'Content-Type': 'text/plain; charset=utf-8' }
            });
          }
        } else {
          // 其他非200非404的错误，例如500等，记录错误，然后继续到next()
          console.error(`[中间件错误] 获取文章静态资源 ${assetFullUrl} 失败，状态码: ${articleResponse.status}。将调用 next()。`);
        }
      } catch (error) {
        // 捕获 env.ASSETS.fetch 或 new URL() 可能抛出的异常
        console.error(`[中间件异常] 为文章ID ${articleId} (路径 ${pathname}) 获取静态资源时发生异常:`, error.message, error.stack);
        // 确保 articleAssetRelativePath 和 url.origin 被正确记录
        console.error(`[中间件异常] 尝试获取的相对路径: ${articleAssetRelativePath}，基准Origin: ${url.origin}`);
        // 出现异常，也应该考虑是返回一个错误页面还是调用 next()。
        // 这里选择调用 next()，让后续的流程处理，或者最终由 Cloudflare Pages 返回标准错误。
        // 如果希望更友好，可以返回一个自定义错误页面，但需注意避免无限循环。
      }
    } else {
      console.log(`[中间件警告] 从路径 ${pathname} 提取的文章ID无效: '${articleId}'。将调用 next()。`);
    }
  } else {
    console.log(`[中间件信息] 非爬虫请求或非文章路径。将传递给下一个处理器。`);
  }

  // 如果以上所有条件都不满足（非爬虫，非文章页，或处理预渲染失败且未提前返回），则调用 next()
  console.log(`[中间件结束] 针对路径 ${pathname} 调用 next()。`);
  return next();
}</code></pre>
            <p>
                这个中间件首先判断 User-Agent 和路径，如果是爬虫访问文章页，就尝试去 <code>/articles/content/</code> 目录下找对应的 <code>.html</code>
                文件。如果找到了 (<code>articleResponse.ok</code>)，就返回这个静态 HTML；如果没找到
                (<code>articleResponse.status === 404</code>)，就尝试返回一个预先准备好的自定义 404 页面。
            </p>

            <h2>完善体验：处理预渲染内容未找到的情况 (404)</h2>
            <p>
                细心的你可能已经注意到，在上面的中间件代码中，当预渲染的文章 HTML (<code>/articles/content/...html</code>) 未找到时 (HTTP 404)，我并没有直接让它回退到
                <code>next()</code>（即显示 SPA 的 <code>index.html</code>）。这样做对 SEO 不太友好，因为爬虫可能会认为这个 URL
                的内容是你的首页，或者是一个无效的页面但返回了 200状态码。
            </p>
            <p>
                <strong>优化方案：</strong> 如果文章的预渲染文件未找到，我们应该：
            </p>
            <ol>
                <li>尝试获取并返回一个你预先准备好的<strong>自定义 404 页面</strong> (例如 <code>/custom/404.html</code>)。</li>
                <li>最重要的是，确保这个响应的 <strong>HTTP 状态码仍然是 404 Not Found</strong>。</li>
            </ol>
            <p>
                这明确地告诉搜索引擎：“这个 URL 对应的具体文章预渲染版本不存在，它是个 404”。这比让爬虫看到一个通用的首页要好得多。我的中间件代码中已经包含了这部分逻辑。你需要确保
                <code>/custom/404.html</code> 这个文件真实存在于你的项目中，并且设计得足够友好。
            </p>

            <h2>锦上添花：SEO 辅助工具与最佳实践</h2>
            <p>
                解决了核心的抓取和渲染问题后，我们还可以做一些其他事情来讨好搜索引擎：
            </p>
            <div class="config-explanation">
                <h4><i class="fas fa-search-plus"></i> SEO 辅助清单</h4>
                <ul>
                    <li>
                        <strong>Meta Description 优化：</strong> 必应曾提示我某些页面的 Meta Description
                        过短。这是一个重要的元标签，它告诉搜索引擎（和用户）页面的简要内容。确保每个重要页面都有独特且描述恰当的 Meta Description，长度适中（通常英文 150-160 字符，中文
                        70-80 汉字左右），我是懒狗没改。
                    </li>
                    <li>
                        <strong>站点地图 (Sitemap - <code>sitemap.xml</code>)：</strong> 站点地图是网站所有重要 URL
                        的列表，可以帮助搜索引擎更快、更全面地发现你的内容。我使用 GitHub Action 自动生成并更新我的站点地图，确保新文章发布后能及时被爬虫知晓。确实省事！
                    </li>
                    <li>
                        <strong><code>robots.txt</code>：</strong> 这个文件告诉爬虫哪些路径可以抓取，哪些不可以。要确保你的 <code>robots.txt</code>
                        配置合理，没有意外地阻止搜索引擎访问重要内容。同时，也可以在里面指定站点地图的位置。
                    </li>
                    <li>
                        <strong>搜索引擎站长工具：</strong> 积极使用 Google Search Console
                        和必应网站管理员工具。它们不仅能告诉你网站的收录情况、错误信息，还能提供很多优化建议。必应的欢迎邮件里通常也会包含一些有用的快速入门链接和工具介绍。
                    </li>
                </ul>
            </div>
            <p>
                我的 <code>robots.txt</code> 文件内容如下，供参考：
            </p>
            <pre><code class="language-plaintext">
# robots.txt for 个人主页
# 这个文件告诉搜索引擎爬虫哪些内容可以访问

User-agent: *

# 允许访问主要内容
Allow: /
Allow: /articles/
Allow: /css/
Allow: /js/
Allow: /images/
Allow: /article/ # 允许访问SPA路由下的文章页，预渲染会处理

# 禁止访问 Cloudflare Functions 内部路径 (如果不想暴露)
# Disallow: /functions/ 
# 根据你的 _middleware.js 逻辑，爬虫实际上不会直接访问 /functions/
# 它们访问的是 /article/xxx，然后由中间件内部处理

# 禁止访问自定义的静态资源目录（例如404页面本身不需要被索引）
Disallow: /custom/

# 禁止访问特定文件类型 (示例)
# Disallow: /*.woff2$
# Disallow: /*.cur$
# Disallow: /*.ani$
# Disallow: /*.crs$

# 网站地图位置
Sitemap: https://blog.zygame1314.site/sitemap.xml

# 爬取延迟（可选，你觉得爬虫不算啥可以不设置）
# Crawl-delay: 1
            </code></pre>
            <div class="article-tips">
                <h3><i class="fas fa-lightbulb"></i> `robots.txt` 注意事项</h3>
                <ul>
                    <li><code>Allow: /article/</code> 是有必要的，因为这是用户和爬虫实际访问的路径。预渲染逻辑是在这个路径被访问时触发的。</li>
                    <li><code>Disallow: /articles/</code> (注意末尾的斜杠) 可能会阻止爬虫访问预渲染内容所在的物理目录，但如果你的预渲染逻辑是从 `env.ASSETS`
                        内部提供，并且不依赖爬虫直接抓取 `/articles/` 目录下的列表页，那么问题不大。关键是确保爬虫能访问到它们需要索引的最终 URL (如
                        <code>/article/my-post</code>)，并且这些 URL 能通过预渲染返回内容。
                    </li>
                    <li>我的 <code>robots.txt</code> 示例中允许了 <code>/articles/</code>
                        是因为有时我可能也会有这个目录下的静态列表页。具体规则需根据你的实际站点结构和需求调整。</li>
                </ul>
            </div>

            <div class="table-container">
                <h3>SEO 优化检查表</h3>
                <table>
                    <thead>
                        <tr>
                            <th>检查项</th>
                            <th>状态/建议</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>CDN 回源配置</td>
                            <td>确保指向 Cloudflare Pages 自定义域，避免隐式重定向。</td>
                        </tr>
                        <tr>
                            <td>动态内容预渲染</td>
                            <td>使用 Cloudflare Functions 中间件为爬虫提供静态 HTML。</td>
                        </tr>
                        <tr>
                            <td><code>env.ASSETS.fetch</code>用法</td>
                            <td>确保使用完整 URL 或合规的 Request 对象。</td>
                        </tr>
                        <tr>
                            <td>404 页面处理</td>
                            <td>为爬虫提供带有 404 状态码的自定义 404 页面。</td>
                        </tr>
                        <tr>
                            <td>Meta Description</td>
                            <td>为每个重要页面撰写独特、相关的描述。</td>
                        </tr>
                        <tr>
                            <td>Sitemap.xml</td>
                            <td>生成并提交到搜索引擎站长工具，保持更新。</td>
                        </tr>
                        <tr>
                            <td>Robots.txt</td>
                            <td>合理配置，确保不阻止重要内容，并指向 Sitemap。</td>
                        </tr>
                        <tr>
                            <td>HTTPS</td>
                            <td>全站使用 HTTPS。Cloudflare Pages 默认支持。</td>
                        </tr>
                        <tr>
                            <td>页面加载速度</td>
                            <td>持续优化。Cloudflare 和 CDN 对此有很大帮助。</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <h2>总结：与“爬虫”握手言和</h2>
            <p>
                经过这一番折腾，它们现在能够正确地抓取和索引我的动态内容了。回顾整个过程，有几点心得体会：
            </p>
            <ul>
                <li><strong>日志是王道：</strong>尤其是在处理像 Cloudflare Functions 这种“黑盒”环境时，详尽的日志输出是定位问题的最有效手段。</li>
                <li><strong>理解平台特性：</strong>深入理解你所使用的平台（如 Cloudflare Pages Functions 的 <code>env.ASSETS.fetch</code>
                    如何工作，中间件的执行流程）至关重要。不能想当然！</li>
                <li><strong>HTTP 状态码很重要：</strong>正确使用 200, 301, 404 等状态码，对 SEO 和用户体验都有积极影响。</li>
                <li><strong>耐心和系统排查：</strong>解决复杂的技术问题往往需要耐心，从表象逐步深入到根源，系统地分析和尝试。</li>
                <li><strong>善用工具：</strong><code>curl</code>、浏览器开发者工具、搜索引擎站长工具，都是我们排查问题的得力助手。</li>
            </ul>
            <p>
                希望我的这次“踩坑”与“填坑”经历，能给同样使用 Cloudflare Pages 或遇到类似 SEO 难题的朋友们带来一些启发。
            </p>

            <div class="article-footer">
                <p>如果你有任何疑问或更好的建议，欢迎在评论区和我交流！一起进步！</p>
            </div>
        </div>
    </main>
</body>

</html>